---
title: "stat_model"
author: "Süleyman Erim, Giacomo Schiavo, Mattia Varagnolo"
date: "`r Sys.Date()`"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax
for authoring HTML, PDF, and MS Word documents. For more details on
using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that
includes both content as well as the output of any embedded R code
chunks within the document. You can embed an R code chunk like this:

```{r message = FALSE}
# import libraries
library(MASS) #for stepAIC (stepwise)
library(tidyverse) # for data manipulation
library(corrplot) # for correlation plot
library(ggplot2) # for plotting
library(gridExtra) # for grid.arrange
library(correlation) # for partial correlation
library(stats) # for anova 
library(car) # for vif package
library(ROCR) # for roc curve
library(ROSE) # for oversampling (just in case)
library(leaps) # for stepwise selection
library(regclass) # for VIF package
library(MLmetrics) # to create confusion matrix
library(pROC) # for ROC Curve
library(e1071) # for Naive Bayes Classifier
library(rpart) # for Decision Tree
library(randomForest) # for Random Forest

```

```{r message=FALSE}

#read data
data = read.csv("data_encoded.csv")
#drop X column
data = data %>% select(-X)
attach(data)
```

```{r}

#train test split
set.seed(123)
train_index = sample(1:nrow(data), 0.8*nrow(data))
# 80% of data is used for training
train = data[train_index,]
# 20% of data is used for testing
test = data[-train_index,]

# Set X and Y, where Y is the target variable "satisfaction"
X_train = train %>% select(-satisfaction)
y_train = train$satisfaction

X_test = test %>% select(-satisfaction)
y_test = test$satisfaction
```

```{r}
# print proportion of satisfied and dissatisfied customers in train and test data
prop.table(table(y_train))
prop.table(table(y_test))
```

## LOGISTIC REGRESSION

```{r}
# Model definition with all features:
glm_full<- glm(data = train,
                satisfaction ~ .,
                family = "binomial")
# summary of full model
s<- summary(glm_full)
s
```

```{r}
#  Here, the code calculates the coefficient of determination, often referred to as R-squared (R²). R² is a measure of how well the GLM model fits the data compared to a null model (a model with no predictors, only an intercept). The formula used to calculate R² is 1 minus the ratio of the deviance of the fitted model (s$deviance) to the deviance of the null model (s$null.deviance). The deviance measures the lack of fit of the model, so a smaller deviance indicates a better fit. Therefore, subtracting the ratio from 1 gives the proportion of deviance explained by the model, which is commonly known as R².
r2<- 1 - (s$deviance/s$null.deviance)

# In this line, the code calculates the variance inflation factor (VIF) by taking the reciprocal of 1 minus the R² value. The VIF is a measure of multicollinearity, which assesses the extent to which predictor variables are correlated with each other. If there is a high degree of multicollinearity, it can affect the reliability and interpretability of the GLM model. The formula used here, 1/(1-r2), is a common way to calculate the VIF, where a value greater than 1 indicates the presence of multicollinearity.
1/(1-r2)
```

#COLLINEARITY #It refers to the situation in which 2 or more predictor
variables are closely #related to one another. The presence of
collinearity can destabilize the model #and make it harder to estimate
the effect of each predictor variable.

#LOW collinearity is desirable.

#VIF (Variance Inflation Factor) is a measure of collinearity among
predictor variables #within a multiple regression. It is calculated by
taking the the ratio of the variance #of all a given model's betas
divide by the variance of a single beta if it were fit alone.

#VIF = 1: no collinearity #1 \< VIF \< 5: moderate collinearity #VIF \>
5: high collinearity (this is the case we want to avoid)

#Depending on what value of VIF you deem to be too high to include in
the model, #you may choose to remove certain predictor variables and see
if the corresponding #R-squared value or standard error of the model is
affected.

```{r}

# VIF Iteration 0 
# Using the VIF function and comparing the obtained values with the
# computed quantity:
# (The process is done iteratively where we delete one variable at time)
vif_values <- VIF(glm_full)



# Create a data frame with variable names and their corresponding VIF values
vif_df <- data.frame(Variable = names(vif_values), VIF = vif_values,row.names = NULL)

# Sort the data frame in decreasing order of VIF values
sorted_df <- vif_df[order(-vif_df$VIF), ]

# Print the sorted data frame
print(sorted_df)
```

```{r}
# VIF Iteration 1
# Using the VIF function and comparing the obtained values with the
# computed quantity:
# (The process is done iteratively where we delete one variable at time)
# Model definition:
glm_vif1<- glm(data = train,
                satisfaction ~ .-Arrival_Delay_in_Minutes,
                family = "binomial")
vif_values <- VIF(glm_vif1)

# Create a data frame with variable names and their corresponding VIF values
vif_df <- data.frame(Variable = names(vif_values), VIF = vif_values,row.names = NULL)

# Sort the data frame in decreasing order of VIF values
sorted_df <- vif_df[order(-vif_df$VIF), ]

# Print the sorted data frame
print(sorted_df)
```

```{r}
# VIF Iteration 2
# Using the VIF function and comparing the obtained values with the
# computed quantity:
# (The process is done iteratively where we delete one variable at time)
# Model definition:
glm_vif2<- glm(data = train,
                satisfaction ~ .
               -Arrival_Delay_in_Minutes
               -Inflight_entertainment,
                family = "binomial")
vif_values <- VIF(glm_vif2)

# Create a data frame with variable names and their corresponding VIF values
vif_df <- data.frame(Variable = names(vif_values), VIF = vif_values,row.names = NULL)

# Sort the data frame in decreasing order of VIF values
sorted_df <- vif_df[order(-vif_df$VIF), ]

# Print the sorted data frame
print(sorted_df)
```

```{r}
# VIF Iteration 3
# Using the VIF function and comparing the obtained values with the
# computed quantity:
# (The process is done iteratively where we delete one variable at time)
# Model definition:
glm_vif3<- glm(data = train,
                satisfaction ~ .
                -Arrival_Delay_in_Minutes
                -Inflight_entertainment
                -Ease_of_Online_booking	,
                family = "binomial")
vif_values <- VIF(glm_vif3)

# Create a data frame with variable names and their corresponding VIF values
vif_df <- data.frame(Variable = names(vif_values), VIF = vif_values,row.names = NULL)

# Sort the data frame in decreasing order of VIF values
sorted_df <- vif_df[order(-vif_df$VIF), ]

# Print the sorted data frame
print(sorted_df)
```

```{r}
# VIF Iteration 4
# Using the VIF function and comparing the obtained values with the
# computed quantity:
# (The process is done iteratively where we delete one variable at time)
# Model definition:
glm_vif4<- glm(data = train,
                satisfaction ~ .
               -Arrival_Delay_in_Minutes
               -Inflight_entertainment
               -Ease_of_Online_booking 
               -Cleanliness	,
                family = "binomial")
vif_values <- VIF(glm_vif4)

# Create a data frame with variable names and their corresponding VIF values
vif_df <- data.frame(Variable = names(vif_values), VIF = vif_values,row.names = NULL)

# Sort the data frame in decreasing order of VIF values
sorted_df <- vif_df[order(-vif_df$VIF), ]

# Print the sorted data frame
print(sorted_df)
```

```{r}
glm_reduced<- glm(data = train,
                satisfaction ~ .
                -Arrival_Delay_in_Minutes
                -Inflight_entertainment
                -Ease_of_Online_booking 
                -Cleanliness	,
                family = "binomial")
# Observation of the model summary:
summary(glm_reduced)
```

```{r}
# While checking the p value in summary, as we see Flight_Distance and Gate_location does not have significant impact on satisfaction.

glm_last<- glm(data = train,
                satisfaction ~ .
                -Arrival_Delay_in_Minutes
                -Inflight_entertainment
                -Ease_of_Online_booking 
                -Cleanliness-Gate_location
                -Flight_Distance
                -Gate_location,
                family = "binomial")


# Computing the predictions with the model on the test set:
pred_glm_last<- predict(glm_last, test, type = "response")
```

# Classification Outputs : Accuracy, F1, Precision, Recall

```{r}
#The function will return the calculated evaluation metrics in the results data frame
calculate_evaluation_metrics <- function(thresholds, output_list, y_test) {
  # Create an empty data frame to store the results
  results_df <- data.frame(
    Threshold = numeric(length(thresholds)),
    Accuracy = numeric(length(thresholds)),
    F1_Score = numeric(length(thresholds)),
    Precision = numeric(length(thresholds)),
    Recall = numeric(length(thresholds))
  )
  
  # Calculate evaluation metrics for each threshold and store the results in the data frame
  for (i in 1:length(thresholds)) {
    threshold <- thresholds[i]
    
    pred_output <- output_list[[as.character(threshold)]]
    
    results_df[i, "Threshold"] <- threshold
    results_df[i, "Accuracy"] <- Accuracy(y_pred = pred_output, y_true = y_test)
    results_df[i, "F1_Score"] <- F1_Score(y_pred = pred_output, y_true = y_test)
    results_df[i, "Precision"] <- Precision(y_pred = pred_output, y_true = y_test)
    results_df[i, "Recall"] <- Recall(y_pred = pred_output, y_true = y_test)
  }
  
  # Format the floating-point numbers with two decimal places
  results_df$Accuracy <- round(results_df$Accuracy, 4)
  results_df$F1_Score <- round(results_df$F1_Score, 4)
  results_df$Precision <- round(results_df$Precision, 4)
  results_df$Recall <- round(results_df$Recall, 4)
  
  return(results_df)
}

```

```{r}

# Converting the prediction in {0,1} according to the chosen threshold:
thresholds <- c(0.4, 0.5, 0.6, 0.7)
output_list <- list()

for (threshold in thresholds) {
  output <- ifelse(pred_glm_last > threshold, 1, 0)
  output_list[[as.character(threshold)]] <- output
}

# Access the outputs using the threshold values as keys
#output_list$`0.4`
#output_list$`0.5`
#output_list$`0.6`
#output_list$`0.7`
```

```{r}
# Calculate evaluation metrics
results <- calculate_evaluation_metrics(thresholds, output_list, y_test)
print(results)

# Print the results as a table in R Markdown
knitr::kable(results, align = "c")

```

```{r}

#Model definition:
# Here we don't re-apply the VIF method because we start from the
# previous result.
glm_full<- glm(data = train,
                satisfaction ~ .,
                family = "binomial")

# Application of the Stepwise method, specifying that we consider
# both the forward and the backward directions. We consider as
# reference metric the Akaike Information Criterion:
glm_step <- stepAIC(glm_full, direction = "both",trace = FALSE)
# Observation of the model summary:
summary(glm_step)

# Computing the predictions with the model on the test set:
pred_glm_step<- predict(glm_step, test, type = "response")
```

```{r}

# Converting the prediction in {0,1} according to the chosen threshold:
thresholds <- c(0.4, 0.5, 0.6, 0.7)
output_list <- list()

for (threshold in thresholds) {
  output <- ifelse(pred_glm_step > threshold, 1, 0)
  output_list[[as.character(threshold)]] <- output
}

# Access the outputs using the threshold values as keys
#output_list$`0.4`
#output_list$`0.5`
#output_list$`0.6`
#output_list$`0.7`
```

```{r}
# Calculate evaluation metrics
results <- calculate_evaluation_metrics(thresholds, output_list, y_test)
print(results)

# Print the results as a table in R Markdown
knitr::kable(results, align = "c")

```

# ROC CURVE

```{r}


plot_roc_curve <- function(predicted_probabilities, true_labels) {
  # Calculate TPR and FPR for different threshold values
  thresholds <- seq(0, 1, by = 0.01)
  tpr <- numeric(length = length(thresholds))
  fpr <- numeric(length = length(thresholds))
  
  for (i in 1:length(thresholds)) {
    threshold <- thresholds[i]
    predicted_labels <- ifelse(predicted_probabilities >= threshold, 1, 0)
    
    tp <- sum(predicted_labels == 1 & true_labels == 1)
    tn <- sum(predicted_labels == 0 & true_labels == 0)
    fp <- sum(predicted_labels == 1 & true_labels == 0)
    fn <- sum(predicted_labels == 0 & true_labels == 1)
    
    tpr[i] <- tp / (tp + fn)
    fpr[i] <- fp / (fp + tn)
  }
  
  # Plot ROC curve
  plot(fpr, tpr, type = "l", main = "ROC Curve", xlab = "False Positive Rate (FPR)", ylab = "True Positive Rate (TPR)")
  abline(0, 1, col = "gray")
  
  # Find the best threshold point
  best_threshold <- thresholds[which.max(tpr - fpr)]
  best_tpr <- tpr[which.max(tpr - fpr)]
  best_fpr <- fpr[which.max(tpr - fpr)]
  
  # Add the best threshold point to the graph
  points(best_fpr, best_tpr, col = "red", pch = 16)
  text(best_fpr, best_tpr, sprintf("Best Threshold (%.2f)", best_threshold), pos = 4)
  
  # Return the best threshold and its TPR value
  return(list(best_threshold = best_threshold, best_tpr = best_tpr))
}


```

```{r}

result<-plot_roc_curve(pred_glm_step, y_test)
print(result$best_threshold)
print(result$best_tpr)
```

## Naive Bayes

```{r}

#1. Estimating the p-dimensional joint distribution of predictors may be
#challenging;
#2. the conditional independence assumption allows us to estimate only
#the marginal distribution of predictors;
#3. in most cases the conditional independence assumption is unrealistic
#and mainly made for convenience...
#4. ... however despite the strong assumption Naive Bayes often
#produces good classification results, especially in settings where n is
#not large enough relative to p to effectively estimate the joint
#distribution of predictors within each class.


#The Naive Bayes Classifier is a probabilistic algorithm used for binary classification. It assumes that features are independent of each other and calculates prior probabilities and likelihoods during the training phase. The prior probabilities represent the occurrence of each class, while the likelihoods determine the probability of a feature given a class. By applying Bayes' theorem, the algorithm calculates posterior probabilities for each class and assigns the instance to the class with the highest probability. 

nb.fit <- naiveBayes(data = X_train,
                     y_train ~ .)

# Make predictions on the test data
predictions <- predict(nb.fit, newdata = X_test)

# Evaluate the performance of the classifier
table(predictions, y_test)
mean(predictions == y_test)
```

## Decision Trees

```{r}
# Build the decision tree model

#choose some variables
# Specify the names of variables to drop
#variables_to_drop <- c("Arrival_Delay_in_Minutes", "Inflight_entertainment", "Ease_of_Online_booking","Gate_location","Flight_Distance","Gate_location")

# Drop the variables from the training dataset
#tree_data <- X_train[, !(names(X_train) %in% variables_to_drop)]


tree_model <- rpart(data = X_train,
                     y_train ~ .,
                    method = "class")

# Display the tree plot
#plot(tree_model)
#text(tree_model)

# Make predictions on the test data
predictions <- predict(tree_model, newdata = X_test, type = "class")

# Evaluate the performance of the classifier
table(predictions, y_test)
mean(predictions == y_test)
```

## Random Forest

```{r}
# Build random forest model

# Build the Random Forest model
# number of trees : 100
rf_model <- randomForest(data = X_train,
                        factor(y_train) ~ ., 
                        ntree = 100)

# Make predictions on the test data
predictions <- predict(rf_model, newdata = X_test,)

# Evaluate the performance of the classifier
table(predictions, y_test)
mean(predictions == y_test)
```

# Training error vs Test error

We are interested in assessing the accuracy of predictions when the
statistical learning method is applied to previously unseen test
observations not used to train the method. Recall the distinction
between the test error and the training error: - The test error is the
expected error that results from using a statistical learning method to
predict the response on a new observation, one that was not used in
training the method. It can be estimated if a designated test set is
available. -In contrast, the training error can be easily computed by
applying the statistical learning method to the observations used in its
training. -But the training error rate often is quite different from the
test error rate, and in particular the former can dramatically
underestimate the latter.

```{r}
# Then we try to reproduce the same plot as above, considering the
# classifications obtained with the models
a <- ggplot(X_test,
            aes(
              x = Flight_Distance ,
              y = Departure_Delay_in_Minutes ,
              color = factor(output_list$`0.6`) # 0 or 1
            )) +
  geom_point(size = 2) +
  labs(
    x = "Flight Distance",
    y = "Departure Delay (minutes)",
    color = "Satisfaction",
    title = "Simple GLM : 0.6"
  ) +
  scale_color_manual(values = c("0" = "red", "1" = "darkgreen")) +
  theme(legend.position = c(0.8, 0.8))

b <- ggplot(X_test,
            aes(
              x = Flight_Distance ,
              y = Departure_Delay_in_Minutes ,
              color = factor(output_list$`0.5`) # 0 or 1
            )) +
  geom_point(size = 2) +
  labs(
    x = "Flight Distance",
    y = "Departure Delay (minutes)",
    color = "Satisfaction",
    title = "Simple GLM  : 0.5"
  ) +
  scale_color_manual(values = c("0" = "red", "1" = "darkgreen")) +
theme(legend.position = c(0.8, 0.8))

grid.arrange(a, b,ncol = 2)
```

```{r}
# We compare the results obtained with the two different models, plotting
# now an estimation of the logistic curve using the predictions given by
# the models:
predicted_data <-
  data.frame(prob.of.Satisfaction = pred_glm_step, Satisfaction = y_test)
predicted_data <-
  predicted_data[order(predicted_data$prob.of.Satisfaction, decreasing = FALSE), ]
predicted_data$rank <- 1:nrow(predicted_data)
a <- ggplot(data = predicted_data, aes(x = rank, y = prob.of.Satisfaction)) +
  geom_point(
    aes(color = as.factor(Satisfaction)),
    alpha = 1,
    shape = 1,
    stroke = 1
  ) +
  xlab("Index") +
  ylab("Predicted probability") +
  ggtitle("Estimated Logistic Curve - Simple GLM")


predicted_data <-
  data.frame(prob.of.Satisfaction = pred_glm_last, Satisfaction = y_test)
predicted_data <-
  predicted_data[order(predicted_data$prob.of.Satisfaction, decreasing = FALSE), ]
predicted_data$rank <- 1:nrow(predicted_data)
b <- ggplot(data = predicted_data, aes(x = rank, y = prob.of.Satisfaction)) +
  geom_point(
    aes(color = as.factor(Satisfaction)),
    alpha = 1,
    shape = 1,
    stroke = 1
  ) +
  xlab("Index") +
  ylab("Predicted probability") +
  ggtitle("Estimated Logistic Curve - GLM with Stepwise")

grid.arrange(a, b, nrow = 2)
```
