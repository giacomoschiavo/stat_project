TV.tmp[1] <- 500
plot(TV.tmp, sales.tmp, pch=16)
mod.out.tmp <- lm(sales.tmp~TV.tmp)
abline(mod.out, col="red")
abline(mod.out.tmp)
par(mfrow=c(2,2))
plot(mod.out.tmp)
par(mfrow=c(1,1))
mod.out <- lm(sales~TV+radio)
summary(mod.out)
set.seed(321)
new.var <- (TV+radio)+rnorm(200)
mod.out2 <- lm(sales~TV+radio+new.var)
summary(mod.out2)
cor(cbind(TV, radio, new.var))
# the package "car" provides the function vif()
library(car)
vif(mod.out)
vif(mod.out2)
library(ISLR2)
data(Auto)
full.mod <- lm(mpg~.-name-origin, data=Auto)
summary(full.mod)
vif(full.mod)
red.mod1 <- lm(mpg~.-origin-name-weight, data=Auto)
summary(red.mod1)
vif(red.mod1)
red.mod2 <- lm(mpg~.-origin-name-weight-displacement, data=Auto)
summary(red.mod2)
vif(red.mod2)
mod.out <- lm(sales ~ TV + radio + newspaper)
summary(mod.out)
mod.out <- lm(sales ~ TV + radio)
summary(mod.out)
par(mfrow=c(2,2))
plot(mod.out)
par(mfrow=c(1,1))
mod.out <- lm(sales ~ TV + radio + TV:radio)
summary(mod.out)
par(mfrow=c(2,2))
plot(mod.out)
par(mfrow=c(1,1))
mod.out <- lm(sales ~  TV*radio*newspaper)
summary(mod.out)
library(faraway)
data(coagulation)
help(coagulation)
attach(coagulation)
summary(coagulation)
boxplot(coag~ diet, col="cyan", ylab="coagulation")
is.factor(diet)
contrasts(diet)
mod.out <- lm(coag~ diet)
summary(mod.out)
# contrasts and design matrix
diet
contrasts(diet)
model.matrix(mod.out)
par(mfrow=c(2,2))
plot(mod.out)
par(mfrow=c(1,1))
library(ISLR2)
data(Auto)
attach(Auto)
mod.out <- lm(mpg ~ origin)
summary(mod.out)
# is origin used as a categorical variable (factor)?
help(Auto)
table(origin)
is.factor(origin)
# convert origin into a factor
origin.f <- factor(origin, levels=1:3,  labels=c("American", "European", "Japanese"))
is.factor(origin.f)
table(origin.f)
# linear model
mod.out <- lm(mpg ~ origin.f)
summary(mod.out)
contrasts(origin.f)
# using two sample test
normtemp <- read.csv("normtemp.txt", sep="", stringsAsFactors = TRUE)
attach(normtemp)
temp.C <- (temperature -32) *5/9
boxplot(temp.C~gender, col="cyan")
var.test(temp.C~gender)
t.test(temp.C~gender, var.equal=TRUE)
# tvalue on genderM is the same as above (same for the pvalue)
contrasts(gender)
mod.out <- lm(temp.C~gender)
summary(mod.out)
# test of homogeneity of variances
bartlett.test(coag~ diet)
# anova
aov.diet <- aov(coag~ diet)
summary(aov.diet)
# post-hoc analysis
tukey.diet <- TukeyHSD(aov.diet)
tukey.diet
plot(tukey.diet)
par(mfrow=c(1, 2))
plot(coag~ diet)
plot(tukey.diet)
par(mfrow=c(1,1))
library(faraway)
data(sexab)
library(faraway)
data(orings)
orings[1,2] <- 1
attach(orings)
linear.out <- lm(damage~temp)
summary(linear.out)
plot(temp, damage, pch=20, xlim=c(45, 85), ylim=c(-.25, 1.25))
abline(linear.out)
new.temp <- data.frame(temp=c(31, 82))
predict(linear.out, newdata=new.temp)
logit.out <- glm(damage ~ temp, family = binomial)
logit.out
summary(logit.out)
plot(temp, damage, pch=20, xlim=c(45, 85), ylim=c(-.25, 1.25))
# function to compute the inverse of the logit
inv.logit <- function(beta0, beta1, x) {
y <- exp(beta0+beta1*x)
return(y/(1+y))
}
x <- seq(45, 85, length=100)
beta.hat <- coefficients(logit.out)
beta0.hat <- beta.hat[1]
beta1.hat <- beta.hat[2]
y <- inv.logit(beta0.hat, beta1.hat, x)
lines(x, y, col="blue", lwd=1.5)
# comparison with the linear model
abline(linear.out)
new.temp <- data.frame(temp=31)
# predict the expected value (probability)
prob.hat <-predict(logit.out, newdata=new.temp, type="response")
prob.hat
# compute the logit
log(prob.hat/(1-prob.hat))
# predict the logit value
logit.hat <-predict(logit.out, newdata=new.temp, type="link")
logit.hat
beta0.hat+31*beta1.hat
library(ISLR2)
data("Default")
attach(Default)
names(Default)
contrasts(default)
mod.out <- glm(default ~ balance, family = binomial)
summary(mod.out)
mod.out <- glm(default ~ income, family = binomial)
summary(mod.out)
mod.out <- glm(default ~ student, family = binomial)
summary(mod.out)
output.F <- glm(default ~ balance+student+income, family = binomial)
summary(output.F)
boxplot(balance ~ student)
boxplot(income ~ student)
output.R <- glm(default ~ balance+student, family = binomial)
anova(output.R, output.F, test="Chisq")
Advertising <- read.csv("Advertising.csv")
attach(Advertising)
# sample size
n <- length(sales)
n
mod.F <- lm(sales~TV+radio+newspaper)
summary(mod.F)
mod.R <- lm(sales~TV)
summary(mod.R)
# comparison of nested models
anova(mod.R, mod.F, test="F")
# where test="F is the default value
anova(mod.R, mod.F)
# NOTE THAT WE CAN OBTAIN THE RSS AS
deviance(mod.F)
deviance(mod.R)
output.F <- glm(damage ~ temp, family = binomial)
summary(output.F)
output.F$deviance
# or, quivalently,
deviance(output.F)
pi.hat <- predict(output.F, type="response")
DF <- -2*sum(damage*log(pi.hat)+(1-damage)*log(1-pi.hat))
DF
df.F <- output.F$df.residual
df.F
output.R <- glm(damage ~ +1, family = binomial)
summary(output.R)
output.R$deviance
df.F <- output.F$df.residual
# degree of freedom for the full model
df.F
output.R <- glm(damage ~ +1, family = binomial)
summary(output.R)
output.R$deviance
pi.hat <- predict(output.R, type="response")
DR <- -2*sum(damage*log(pi.hat)+(1-damage)*log(1-pi.hat))
DR
df.R <- output.R$df.residual
# degree of freedom for the reduced model
df.R
dev.test <- DR-DF
dev.test
df <- df.R- df.F
df
pvalue <- 1-pchisq(dev.test, df)
pvalue
anova(output.R, output.F, test="Chisq")
output.R <- glm(damage ~ +1, family = binomial)
summary(output.R)
output.R$deviance
pi.hat <- predict(output.R, type="response")
DR <- -2*sum(damage*log(pi.hat)+(1-damage)*log(1-pi.hat))
DR
df.R <- output.R$df.residual
# degree of freedom for the reduced model
df.R
dev.test <- DR-DF
dev.test
df <- df.R- df.F
# deg of freedom for the deviance difference test
df
pvalue <- 1-pchisq(dev.test, df)
pvalue
anova(output.R, output.F, test="Chisq")
logit.out.resid <- residuals(output.F, type="pearson")
plot(logit.out.resid~fitted(output.F))
abline(h=0, lty=3)
qqnorm(logit.out.resid)
qqline(logit.out.resid)
logit.out.resid <- residuals(output.F, type="pearson")
plot(logit.out.resid~fitted(output.F))
abline(h=0, lty=3)
qqnorm(logit.out.resid)
qqline(logit.out.resid)
par(mfrow=c(2,2))
plot(output.F)
par(mfrow=c(1,1))
library(ISLR2)
data("Default")
attach(Default)
l <- default=="Yes"
plot(balance, income, col=l+1, pch=l*15+1)
mod.out <- glm(default ~ income + balance, family = binomial)
logistic.prob <- predict(mod.out, type="response")
logistic.pred <- rep("No", 10000)
logistic.pred[logistic.prob>0.5] <- "Yes"
default[200:215]
logistic.pred[200:215]
default[200:215]==logistic.pred[200:215]
summary(mod.out)
beta <- coefficients(mod.out)
intercept <- -beta[1]/beta[2]
slope <- -beta[3]/beta[2]
plot(balance, income, col=l+1, pch=l*15+1)
abline(intercept, slope, lwd=2, col="blue")
table(logistic.pred, default)
# overall (training) error rate
(225+38)/10000
trivial.pred <- rep("No", 10000)
table(trivial.pred, default)
# overall (training) error rate for the trivial classifier
333/10000
# (training) error rate for defaulting-customers
table(logistic.pred, default)
225/(225+108)
logistic.pred <- rep("No", 10000)
logistic.pred[logistic.prob>0.2] <- "Yes"
table(logistic.pred, default)
# overall (training) error rate
(133+271)/10000
# (training) error rate among individuals who defaulted
133/(133+200)
logistic.pred <- rep("No", 10000)
logistic.pred[logistic.prob>0.5] <- "Yes"
CM <- table(logistic.pred, default)
# rearrange rows and columns to match
# the confusion matrix on the slides
CM <- CM[2:1, 2:1]
CM <- addmargins(CM, margin = c(1, 2))
CM
fpr <- 38/9667
fpr
fpr <- CM["Yes", "No"]/CM["Sum", "No"]
fpr
specif <- 1 - fpr
specif
tpr <- 108/333
tpr
tpr <- CM["Yes", "Yes"]/CM["Sum", "Yes"]
tpr
library(pROC)
# The two groups are labelled as cases ad controls
# and about direction if, for example, "controls < cases"
# then observations with estimated probability smaller than
# the given threshold are classified as "controls/negative"
#
# If labels are "0" and "1" then
# "0" = controls (i.e. negative)
# "1" = cases (i.e. positive)
#
roc.out <- roc(default, logistic.prob)
roc.out <- roc(default, logistic.prob, levels=c("No", "Yes"))
# argument "legacy.axes" is a logical indicating if the specificity axis
# (x axis) must be plotted as as decreasing “specificity” (FALSE, the default)
# or increasing “1 - specificity” (TRUE)
plot(roc.out)
plot(roc.out, legacy.axes=TRUE)
plot(roc.out,  print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")
auc(roc.out)
plot(roc.out,  print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")
# specificity and sensitivity for a given threshold
coords(roc.out, x=0.2)
# threshold that maximizes the sum of sensitivity and specificity
coords(roc.out, x="best")
library(ISLR2)
names(Smarket)
dim(Smarket)
summary(Smarket)
pairs(Smarket)
cor(Smarket)
library(pROC)
roc.out <- roc(Direction, glm.probs, levels=c("Down", "Up"))
glm.fits <- glm(Direction~Lag1+Lag2,data=Smarket,family=binomial)
glm.probs <- predict(glm.fits,type="response")
glm.pred <- rep("Down",1250)
glm.pred[glm.probs>.5]="Up"
table(glm.pred,Direction)
# overall error rate
(102+488)/1250
table(Direction)/length(Direction)
table(glm.pred,Direction)
library(ISLR2)
names(Smarket)
dim(Smarket)
summary(Smarket)
pairs(Smarket)
cor(Smarket)
attach(Smarket)
plot(Volume, xlab="time")
glm.fits <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=Smarket, family=binomial)
# alternative syntax using the "dot"
glm.fits <- glm(Direction~.-Today-Year, data=Smarket, family=binomial)
summary(glm.fits)
# obtain fitted probabilities
glm.probs <- predict(glm.fits, type="response")
glm.probs[1:10]
# check the coding of Direction (to properly interpret probabilities)
contrasts(Direction)
glm.pred <- rep("Down",1250)
glm.pred[glm.probs>.5] <- "Up"
table(glm.pred,Direction)
# overall (training) error rate
(141+457)/1250
# proportions of "Up" and "Down"
table(Direction)/length(Direction)
library(pROC)
roc.out <- roc(Direction, glm.probs, levels=c("Down", "Up"))
plot(roc.out, print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")
glm.fits <- glm(Direction~Lag1+Lag2,data=Smarket,family=binomial)
glm.probs <- predict(glm.fits,type="response")
glm.pred <- rep("Down",1250)
glm.pred[glm.probs>.5]="Up"
table(glm.pred,Direction)
# overall error rate
(102+488)/1250
table(Direction)/length(Direction)
train <- (Year<2005)
names(Smarket)
help(Smarket)
train <- (Year<2005)
Smarket.2005 <- Smarket[!train,]
dim(Smarket.2005)
Direction.2005 <- Direction[!train]
Direction.2005
glm.fits <- glm(Direction~.-Today-Year, family=binomial, data=Smarket, subset=train)
# prediction on hold-out set
glm.probs <- predict(glm.fits,Smarket.2005,type="response")
glm.pred <- rep("Down",252)
glm.pred[glm.probs>.5] <- "Up"
table(glm.pred,Direction.2005)
(97+34)/252
# ROC curve
roc.out <- roc(Direction.2005, glm.probs, levels=c("Up", "Down"))
plot(roc.out,  print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")
library(pROC)
data(Auto)
mpg01 <- (Auto$mpg > median(Auto$mpg))+0
table(mpg01)
new.auto <- data.frame(mpg01, Auto[, -c(1,9)])
new.auto$origin <- as.factor(new.auto$origin)
rm(mpg01)
mpg01 <- (Auto$mpg > median(Auto$mpg))+0
table(mpg01)
new.auto <- data.frame(mpg01, Auto[, -c(1,9)])
new.auto$origin <- as.factor(new.auto$origin)
rm(mpg01)
mpg01
new.auto <- data.frame(mpg01, Auto[, -c(1,9)])
new.auto$origin <- as.factor(new.auto$origin)
rm(mpg01)
n <- dim(new.auto)[1]
n
attach(new.auto)
names(new.auto)
par(mfrow=c(2, 3))
boxplot(cylinders ~ mpg01)
boxplot( displacement~ mpg01)
boxplot(horsepower ~ mpg01)
boxplot(weight ~ mpg01)
boxplot(acceleration ~ mpg01)
boxplot(year ~ mpg01)
par(mfrow=c(1,1))
set.seed(123)
set.seed(123)
validation <- sample(1:n, 130)
train <- setdiff(1:n, validation)
validation.auto <- new.auto[validation, ]
train.auto <- new.auto[train, ]
glm.fit.all <- glm(mpg01~year, data=new.auto, family=binomial)
glm.prob.all <- predict(glm.fit.all, type="response")
glm.pred.all <- (glm.prob.all>0.5)+0
mean(mpg01==glm.pred.all)
mean(mpg01!=glm.pred.all)
glm.fit.train <- glm(mpg01~year, data=train.auto, family=binomial)
glm.prob.train <- predict(glm.fit.train, validation.auto, type="response")
glm.pred.train <- (glm.prob.train>0.5)+0
mean(validation.auto$mpg01==glm.pred.train)
mean(validation.auto$mpg01!=glm.pred.train)
library(ISLR2)
attach(Smarket)
train <- (Year<2005)
install.packages("tidyverse")
install.packages("corrplot")
install.packages("gridExtra")
install.packages("reshape2")
# import libraries
library(tidyverse)
library(corrplot)
library(ggplot2)
library(gridExtra)
library(correlation)
library(reshape)
library(reshape2)
data_train = read.csv("train.csv")
data_test = read.csv("test.csv")
# merge train and test data
data = rbind(data_train, data_test)
setwd("C:/Users/giaco/Desktop/sl/project")
data_train = read.csv("train.csv")
data_test = read.csv("test.csv")
# merge train and test data
data = rbind(data_train, data_test)
attach(data)
par(mfrow = c(1, 1))
# DATA PREPROCESSING
# replace dots with underscores in column names
names(data) = gsub("\\.", "_", names(data))
# drop X and id column
#TODO: explain why
data = data %>% select(-X, -id)
# convert gender to numeric and then to factor
data$Gender = as.numeric(as.factor(data$Gender)) -1
# change type of customer to 0 and disloyal customer to 1
data$Customer_Type = as.numeric(factor(data$Customer_Type, levels = c("Loyal Customer", "disloyal Customer"))) - 1
# change type of tr avel to 0 and personal travel to 1
data$Type_of_Travel = as.numeric(factor(data$Type_of_Travel, levels = c("Personal Travel", "Business travel"))) - 1
# change class Business is 2, Eco Plus is 1 and Eco is 0
data$Class = as.numeric(factor(data$Class, levels = c("Business", "Eco Plus", "Eco"))) - 1
data$satisfaction = as.numeric(factor(data$satisfaction, levels = c("neutral or dissatisfied", "satisfied"))) - 1
# drop na values in Arrival Delay in Minutes
# TODO: explain why (now it's dropped to simplify the analysis)
data = data %>% drop_na(Arrival_Delay_in_Minutes)
sat = data$satisfaction
features_names = names(data)
num_cols = 4
num_rows = ceiling(ncol(data)/num_cols)
par(mfrow = c(num_rows, num_cols))
# for each feature plot the density of satisfied and dissatisfied customers
for(col in features_names) {
# calculate number of breaks
num_breaks = length(unique(data[[col]]))
num_breaks = min(num_breaks, 20)
hist_data = hist(data[[col]], breaks = num_breaks,
main = paste("Histogram of ", col), xlab = col, ylab = "Frequency",
col = "lightblue"
)
}
# DATA BALANCE: quite balanced
prop.table(table(data$satisfaction))
# for each feature plot the density of satisfied and dissatisfied customers
for(col in features_names) {
# calculate number of breaks
num_breaks = length(unique(data[[col]]))
num_breaks = min(num_breaks, 20)
hist_data = hist(data[[col]], breaks = num_breaks,
main = paste("Histogram of ", col), xlab = col, ylab = "Frequency",
col = "lightblue"
)
}
# Train-test split
set.seed(123)
train_index = sample(1:nrow(data), 0.8*nrow(data))
# 80% of data is used for training
train = data[train_index,]
# 20% of data is used for testing
test = data[-train_index,]
# Create a histogram with different colors for each category
ggplot(data, aes(x = satisfaction, fill = factor(Customer_Type))) +
geom_histogram(binwidth = 0.5, position = "dodge") +
scale_fill_manual(values = rainbow(length(unique(data$Customer_Type))),
labels = unique(data$Customer_Type),
name = "Customer Type") +
labs(title = "Histogram of Satisfaction by Category", x = "Satisfaction", y = "Count")
